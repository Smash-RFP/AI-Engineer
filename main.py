import os
import argparse
from tqdm import tqdm


from src.generator.llm_generator import generate_response
from src.loader.preprocessing import extract_text_split_virtual_pages, sanitize_filename, save_chunks_as_jsonl
from src.vectordb.vectordb import check_api_keys, run

from src.retrieval.modules.bm25_docs_generate import generate_bm25_docs
from src.retrieval.modules.retrieved_contexts import run_retrieve

def process_single_pdf(pdf_path, output_dir, threshold=1.0):
    print(f"ğŸ” {pdf_path} ì²˜ë¦¬ ì¤‘...")
    chunks = extract_text_split_virtual_pages(pdf_path, threshold)
    source_id = sanitize_filename(pdf_path)
    output_path = os.path.join(output_dir, f"{source_id}.jsonl")
    save_chunks_as_jsonl(chunks, source_id, output_path)
    print(f" {source_id}.jsonl ì €ì¥ ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬\n")

def run_batch_pipeline(input_dir, output_dir, threshold=1.0):
    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".pdf")]
    if not pdf_files:
        print(" PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    print(f" ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘\n")
    for file in tqdm(pdf_files):
        pdf_path = os.path.join(input_dir, file)
        try:
            process_single_pdf(pdf_path, output_dir, threshold)
        except Exception as e:
            print(f" {file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

username = "eojin-kim"

input_pdf_dir = f"/home/{username}/AI-Engineer/data"
output_jsonl_dir = f"/home/{username}/AI-Engineer/data/dummy"

DEFAULT_DUMMY_DATA_DIR = f"/home/{username}/AI-Engineer/data/dummy"
DEFAULT_CHROMA_DB_DIR = "./data/chroma_db"
COLLECTION_NAME = "rfp_documents"
EMBEDDING_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

def continue_response(QUERY:str, previous_response_id=None):
    run_retrieve(QUERY)
    contexts = run_retrieve(QUERY)
    response_text, previous_response_id = generate_response(query=QUERY, retrieved_rfp_text=contexts, previous_response_id=previous_response_id)
    print('response_text: ', response_text)

    return response_text, previous_response_id

#  ì‹¤í–‰
if __name__ == "__main__":
    # run_batch_pipeline(input_pdf_dir, output_jsonl_dir, threshold=1.0)

    # parser = argparse.ArgumentParser(description="JSONL íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    # parser.add_argument("--data_dir", type=str, default=DEFAULT_DUMMY_DATA_DIR, help="ì…ë ¥ JSONL íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    # parser.add_argument("--db_dir", type=str, default=DEFAULT_CHROMA_DB_DIR, help="ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    # parser.add_argument("--rebuild", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
    
    # args = parser.parse_args()
    
    # check_api_keys()
    # run(args.data_dir, args.db_dir, args.rebuild)
    
    # generate_bm25_docs(
    #     input_dir=output_jsonl_dir,
    #     output_pkl_path="data/bm25_docs.pkl",
    #     output_map_path="data/bm25_chunk_id_map.json"
    # )
    
    # retrieval
    response_text, previous_response_id = continue_response("êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ì´ ë°œì£¼í•œ ì´ëŸ¬ë‹ì‹œìŠ¤í…œ ê´€ë ¨ ì‚¬ì—… ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ ì¤˜.")
    
    """
    ëŒ€í™”ë¥¼ ì´ì–´í•˜ëŠ” ë°©ë²•
    ex)
    response_text, previous_response_id = continue_response("ì½˜í…ì¸  ê°œë°œ ê´€ë¦¬ ìš”êµ¬ ì‚¬í•­ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì•Œë ¤ ì¤˜." , previous_response_id)
    response_text, previous_response_id = continue_response("êµìœ¡ì´ë‚˜ í•™ìŠµ ê´€ë ¨í•´ì„œ ë‹¤ë¥¸ ê¸°ê´€ì´ ë°œì£¼í•œ ì‚¬ì—…ì€ ì—†ë‚˜?" , previous_response_id)
    ...

    """
    response_text, previous_response_id = continue_response("ì½˜í…ì¸  ê°œë°œ ê´€ë¦¬ ìš”êµ¬ ì‚¬í•­ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì•Œë ¤ ì¤˜." , previous_response_id)
    response_text, previous_response_id = continue_response("êµìœ¡ì´ë‚˜ í•™ìŠµ ê´€ë ¨í•´ì„œ ë‹¤ë¥¸ ê¸°ê´€ì´ ë°œì£¼í•œ ì‚¬ì—…ì€ ì—†ë‚˜?" , previous_response_id)
