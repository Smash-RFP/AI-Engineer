import os
import sys
import json
import shutil
import argparse
from tqdm import tqdm

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document

from src.generator.experiment.eval_performance import eval_llm
from src.generator.llm_generator import generate_response
from src.loader.preprocessing import extract_text_split_virtual_pages, sanitize_filename, save_chunks_as_jsonl
from src.vectordb.vectordb import check_api_keys, load_and_parse_documents, add_documents_in_batches, save_chunk_id_mapping, run

from src.retrieval.modules.bm25_docs_generate import generate_bm25_docs
from src.retrieval.modules.retrieved_contexts import run_retrieve

def process_single_pdf(pdf_path, output_dir, threshold=1.0):
    print(f"ğŸ” {pdf_path} ì²˜ë¦¬ ì¤‘...")
    chunks = extract_text_split_virtual_pages(pdf_path, threshold)
    source_id = sanitize_filename(pdf_path)
    output_path = os.path.join(output_dir, f"{source_id}.jsonl")
    save_chunks_as_jsonl(chunks, source_id, output_path)
    print(f" {source_id}.jsonl ì €ì¥ ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬\n")

def run_batch_pipeline(input_dir, output_dir, threshold=1.0):
    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".pdf")]
    if not pdf_files:
        print(" PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    print(f" ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘\n")
    for file in tqdm(pdf_files):
        pdf_path = os.path.join(input_dir, file)
        try:
            process_single_pdf(pdf_path, output_dir, threshold)
        except Exception as e:
            print(f" {file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

username = "gcp-JeOn"

input_pdf_dir = f"/home/{username}/AI-Engineer/data"
output_jsonl_dir = f"/home/{username}/AI-Engineer/data/dummy"

DEFAULT_DUMMY_DATA_DIR = f"/home/{username}/AI-Engineer/data/dummy"
DEFAULT_CHROMA_DB_DIR = "./data/chroma_db"
COLLECTION_NAME = "rfp_documents"
EMBEDDING_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

#  ì‹¤í–‰
if __name__ == "__main__":
    run_batch_pipeline(input_pdf_dir, output_jsonl_dir, threshold=1.0)

    parser = argparse.ArgumentParser(description="JSONL íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument("--data_dir", type=str, default=DEFAULT_DUMMY_DATA_DIR, help="ì…ë ¥ JSONL íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--db_dir", type=str, default=DEFAULT_CHROMA_DB_DIR, help="ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--rebuild", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()
    
    check_api_keys()
    run(args.data_dir, args.db_dir, args.rebuild)
    
    generate_bm25_docs(
        input_dir=output_jsonl_dir,
        output_pkl_path="data/bm25_docs.pkl",
        output_map_path="data/bm25_chunk_id_map.json"
    )
    
    # retrieval
    QUERY = "í•´ì™¸ ì§€ì‹ ì¬ì‚° ì„¼í„° ì‚¬ì—… ê´€ë¦¬ ì‹œìŠ¤í…œ ê¸°ëŠ¥ ê°œë°œ ì…ì°° ì°¸ê°€ ìê²©"
    contexts = run_retrieve(QUERY)

    # response_text, previous_response_id = generate_response(query=QUERY, retrieved_rfp_text=contexts)
    
    # ëŒ€í™” ì´ì–´ì„œ í•˜ë ¤ë©´ previous_response_id íŒŒë¼ë¯¸í„°ë¡œ ë„£ì–´ì¤Œ.
    # response_text, previous_response_id = generate_response(query=QUERY, retrieved_rfp_text=contexts, previous_response_id=previous_response_id)

