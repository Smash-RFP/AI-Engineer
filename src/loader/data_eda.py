import os
import re
import glob
import json
import fitz
import pdfplumber
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib as mpl
from tqdm import tqdm


def setup_korean_font():
    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
    if not os.path.exists(font_path):
        print("âš ï¸ í•œê¸€ í°íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return
    fm.fontManager.addfont(font_path)
    font_prop = fm.FontProperties(fname=font_path)
    mpl.rcParams['font.family'] = font_prop.get_name()
    mpl.rcParams['axes.unicode_minus'] = False


def count_pages_adjusting_for_two_column(pdf_path, threshold_ratio=1.0):
    try:
        doc = fitz.open(pdf_path)
        real_page_count = 0
        has_two_column_page = False
        for page in doc:
            ratio = page.rect.width / page.rect.height
            if ratio > threshold_ratio:
                real_page_count += 2
                has_two_column_page = True
            else:
                real_page_count += 1
        return real_page_count, has_two_column_page
    except Exception as e:
        print(f" í˜ì´ì§€ ìˆ˜ ì˜¤ë¥˜: {pdf_path} - {e}")
        return 0, False


def extract_text_and_count_chars(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        return sum(len(page.get_text()) for page in doc)
    except Exception as e:
        print(f" ê¸€ì ìˆ˜ ì˜¤ë¥˜: {pdf_path} - {e}")
        return 0


def classify_rfp(filename, category_map):
    name = filename.replace(".pdf", "").strip()
    parts = name.split("_")
    org = parts[0].strip() if len(parts) > 1 else "ë¯¸ìƒ"
    large = next((category for keyword, category in category_map.items() if keyword in org), "ê¸°íƒ€")
    year_match = re.search(r"(20\d{2})", name)
    year = year_match.group(1) if year_match else "ë¯¸ìƒ"
    keyword_candidates = ["ê³ ë„í™”", "êµ¬ì¶•", "ìš´ì˜", "ê°œì„ ", "ì»¨ì„¤íŒ…", "ìœ ì§€ë³´ìˆ˜", "ì¬êµ¬ì¶•", "ê¸°ëŠ¥ê°œì„ ", "ì‹œìŠ¤í…œ", "í”Œë«í¼", "ERP", "ISP"]
    keywords = ", ".join([kw for kw in keyword_candidates if kw in name]) or "ê¸°íƒ€"
    return {"ëŒ€ë¶„ë¥˜": large, "ì¤‘ë¶„ë¥˜(ê¸°ê´€ëª…)": org, "ì—°ë„": year, "ì‚¬ì—… í‚¤ì›Œë“œ": keywords, "íŒŒì¼ëª…": name}


def analyze_table_counts(pdf_paths):
    results = []
    for pdf_path in tqdm(pdf_paths, desc="ğŸ“„ í…Œì´ë¸” ìˆ˜ ë¶„ì„ ì¤‘"):
        file = os.path.basename(pdf_path)
        table_count = 0
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    table_count += len(page.extract_tables())
        except Exception as e:
            print(f"âš ï¸ {file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
        results.append({"filename": file, "num_tables": table_count})
    return pd.DataFrame(results)


def analyze_jsonl_chunks(jsonl_dir, output_dir):
    jsonl_files = glob.glob(os.path.join(jsonl_dir, "*.jsonl"))
    if not jsonl_files:
        print("âš ï¸ JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_chunks = 0
    total_keywords = 0
    file_count = 0
    chunks_per_file = []

    for jsonl_file in tqdm(jsonl_files, desc="ğŸ“‚ JSONL ì²­í¬ ë¶„ì„ ì¤‘"):
        try:
            with open(jsonl_file, "r", encoding="utf-8") as f:
                chunks = [json.loads(line) for line in f if line.strip()]
                chunk_count = len(chunks)
                keyword_count = sum(len(chunk.get("metadata", {}).get("key_word", [])) for chunk in chunks)

                total_chunks += chunk_count
                total_keywords += keyword_count
                chunks_per_file.append(chunk_count)
                file_count += 1
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {jsonl_file} - {e}")
            continue

    if file_count == 0:
        print("âŒ ìœ íš¨í•œ JSONL íŒŒì¼ ì—†ìŒ")
        return

    avg_chunks_per_file = total_chunks / file_count
    avg_keywords_per_chunk = total_keywords / total_chunks if total_chunks > 0 else 0

    print("\nğŸ“Š JSONL ì²­í¬ í†µê³„ ìš”ì•½")
    print(f" ì´ JSONL íŒŒì¼ ìˆ˜: {file_count}")
    print(f" í‰ê·  ì²­í¬ ìˆ˜ (íŒŒì¼ë‹¹): {avg_chunks_per_file:.2f}")
    print(f" í‰ê·  í‚¤ì›Œë“œ ìˆ˜ (ì²­í¬ë‹¹): {avg_keywords_per_chunk:.2f}")

    # ğŸ”½ ì²­í¬ ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(10, 6))
    bins = range(0, max(chunks_per_file) + 5, 5)
    plt.hist(chunks_per_file, bins=bins, color="mediumpurple", edgecolor="black", rwidth=0.9)
    plt.title("JSONL íŒŒì¼ë‹¹ ì²­í¬ ìˆ˜ ë¶„í¬", fontsize=14)
    plt.xlabel("ì²­í¬ ìˆ˜", fontsize=12)
    plt.ylabel("íŒŒì¼ ìˆ˜", fontsize=12)
    plt.xticks(bins)
    plt.grid(axis="y", linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "jsonl_ì²­í¬ìˆ˜_íˆìŠ¤í† ê·¸ë¨.png"))
    plt.close()


def run_eda_pipeline(pdf_dir, output_dir, jsonl_dir=None):
    setup_korean_font()
    os.makedirs(output_dir, exist_ok=True)
    pdf_files = glob.glob(os.path.join(pdf_dir, "**/*.pdf"), recursive=True)

    # ì‹¤ì§ˆ í˜ì´ì§€ ìˆ˜ ë° ê¸€ì ìˆ˜ ë¶„ì„
    adjusted_page_counts = []
    char_counts = []
    two_column_pdf_count = 0
    for path in tqdm(pdf_files, desc="ğŸ“„ ì‹¤ì§ˆ í˜ì´ì§€/ê¸€ì ìˆ˜ ë¶„ì„"):
        pages, has_two_col = count_pages_adjusting_for_two_column(path)
        chars = extract_text_and_count_chars(path)
        adjusted_page_counts.append(pages)
        char_counts.append(chars)
        if has_two_col:
            two_column_pdf_count += 1

    print("\nğŸ“Š PDF í†µê³„ ìš”ì•½")
    print(f" ì´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}")
    print(f" í‰ê·  ì‹¤ì§ˆ í˜ì´ì§€ ìˆ˜ (2ë‹¨ í¬í•¨): {np.mean(adjusted_page_counts):.2f}")
    print(f" í‰ê·  ê¸€ì ìˆ˜: {np.mean(char_counts):.2f}")
    print(f" 2ë‹¨ êµ¬ì„± PDF ìˆ˜: {two_column_pdf_count}")
    print(f" ìµœëŒ€ ê¸€ì ìˆ˜: {np.max(char_counts)}")
    print(f" ìµœì†Œ ê¸€ì ìˆ˜: {np.min(char_counts)}")

    # íˆìŠ¤í† ê·¸ë¨ ì €ì¥
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.hist(adjusted_page_counts, bins=20, color='skyblue', edgecolor='black')
    plt.title("PDF í˜ì´ì§€ ìˆ˜ ë¶„í¬ (2ë‹¨ ë³´ì • í¬í•¨)")
    plt.xlabel("ì‹¤ì§ˆ í˜ì´ì§€ ìˆ˜")
    plt.ylabel("íŒŒì¼ ìˆ˜")

    plt.subplot(1, 2, 2)
    plt.hist(char_counts, bins=20, color='lightgreen', edgecolor='black')
    plt.title("PDF ê¸€ì ìˆ˜ ë¶„í¬")
    plt.xlabel("ê¸€ì ìˆ˜")
    plt.ylabel("íŒŒì¼ ìˆ˜")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "í˜ì´ì§€_ê¸€ììˆ˜_íˆìŠ¤í† ê·¸ë¨.png"))
    plt.close()

    # íŒŒì¼ëª… ê¸°ë°˜ RFP ë¶„ë¥˜
    category_map = {
        "ëŒ€í•™êµ": "ëŒ€í•™êµ", "ëŒ€í•™": "ëŒ€í•™êµ", "í•™êµ": "ëŒ€í•™êµ",
        "ê³µì‚¬": "ê³µê¸°ì—…", "ê³µë‹¨": "ê³µê¸°ì—…",
        "ì¬ë‹¨ë²•ì¸": "ê³µê³µê¸°ê´€", "ì¬ë‹¨": "ê³µê³µê¸°ê´€",
        "í˜‘íšŒ": "í˜‘íšŒ/ë‹¨ì²´", "ì§„í¥ì›": "ì§„í¥ê¸°ê´€",
        "ì§€ì›ì„¼í„°": "ì§€ì›ì„¼í„°", "ì—°êµ¬ì›": "ì—°êµ¬ê¸°ê´€",
        "ìœ„ì›íšŒ": "ì •ë¶€ê¸°ê´€", "ì²­": "ì •ë¶€ê¸°ê´€", "ë¶€": "ì •ë¶€ê¸°ê´€",
        "ê´‘ì—­ì‹œ": "ì§€ìì²´", "íŠ¹ë³„ì‹œ": "ì§€ìì²´", "ì‹œì²­": "ì§€ìì²´", "ë„ì²­": "ì§€ìì²´", "êµ°ì²­": "ì§€ìì²´"
    }

    rfp_files = [os.path.basename(f) for f in pdf_files]
    df_rfp = pd.DataFrame([classify_rfp(f, category_map) for f in rfp_files])

    # ì €ì¥
    rfp_output_path = os.path.join(output_dir, "RFP_ë¶„ë¥˜_ê²°ê³¼.xlsx")
    df_rfp.to_excel(rfp_output_path, index=False)
    print(f"\nâœ… RFP ë¶„ë¥˜ ê²°ê³¼ ì €ì¥: {rfp_output_path}")
    print(df_rfp.to_string(index=False))

    # ë¶„í¬ ì‹œê°í™” ì €ì¥
    distribution_table = pd.crosstab(df_rfp["ì‚¬ì—… í‚¤ì›Œë“œ"], df_rfp["ëŒ€ë¶„ë¥˜"])
    top_keywords = distribution_table.sum(axis=1).sort_values(ascending=False).head(10).index
    top_distribution = distribution_table.loc[top_keywords]

    plt.figure(figsize=(12, 6))
    top_distribution.plot(kind="bar", stacked=True, colormap="tab20", figsize=(12, 6))
    plt.title("ìƒìœ„ 10ê°œ RFP ì‚¬ì—… í‚¤ì›Œë“œì˜ ëŒ€ë¶„ë¥˜ë³„ ë¶„í¬")
    plt.xlabel("ì‚¬ì—… í‚¤ì›Œë“œ")
    plt.ylabel("RFP ê±´ìˆ˜")
    plt.xticks(rotation=45)
    plt.legend(title="ê¸°ê´€ ëŒ€ë¶„ë¥˜", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "RFP_í‚¤ì›Œë“œ_ë¶„í¬.png"))
    plt.close()

    # í…Œì´ë¸” ìˆ˜ ë¶„ì„
    df_tables = analyze_table_counts(pdf_files)
    print(f"\nğŸ“ˆ í‰ê·  í…Œì´ë¸” ìˆ˜: {df_tables['num_tables'].mean():.2f}")

    bins = range(0, df_tables["num_tables"].max() + 20, 20)
    plt.figure(figsize=(10, 6))
    plt.hist(df_tables["num_tables"], bins=bins, color="salmon", edgecolor="black", rwidth=0.9)
    plt.title("PDFë³„ í…Œì´ë¸” ìˆ˜ ë¶„í¬ (20ê°œ ë‹¨ìœ„ êµ¬ê°„)", fontsize=14)
    plt.xlabel("PDF ë‚´ í…Œì´ë¸” ìˆ˜ ë²”ìœ„", fontsize=12)
    plt.ylabel("PDF íŒŒì¼ ê°œìˆ˜", fontsize=12)
    plt.xticks(bins)
    plt.grid(axis="y", linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "í…Œì´ë¸”_ìˆ˜_ë¶„í¬.png"))
    plt.close()

    # JSONL ë¶„ì„ (ì˜µì…˜)
    if jsonl_dir:
        analyze_jsonl_chunks(jsonl_dir, output_dir)