{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707705dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_teddynote.document_loaders import HWPLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94aab0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ íŒŒì¼ ìˆ˜: 101\n",
      "PDF íŒŒì¼ ìˆ˜: 4\n",
      "HWP íŒŒì¼ ìˆ˜: 96\n"
     ]
    }
   ],
   "source": [
    "pdf_files = []\n",
    "hwp_files = []\n",
    "\n",
    "# íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ\n",
    "data_dir = \"/home/daeseok/AI-Engineer/data\"\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_files.append(filename)\n",
    "    elif filename.endswith(\".hwp\"):\n",
    "        hwp_files.append(filename)    \n",
    "        \n",
    "print(f\"ì´ íŒŒì¼ ìˆ˜: {len(os.listdir(data_dir))}\")\n",
    "print(f\"PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\")\n",
    "print(f\"HWP íŒŒì¼ ìˆ˜: {len(hwp_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "536ece22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "dummy_dir = os.path.join(data_dir, \"dummy\")\n",
    "os.makedirs(dummy_dir, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼ ëª©ë¡ ë¶„ë¥˜\n",
    "pdf_files = [f for f in os.listdir(data_dir) if f.endswith(\".pdf\")][:4]\n",
    "hwp_files = [f for f in os.listdir(data_dir) if f.endswith(\".hwp\")][:4]\n",
    "\n",
    "# ì²­í‚¹ ì„¸íŒ…\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    #separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def process_and_save(file_path, loader_class):\n",
    "    loader = loader_class(file_path)\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    result = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        result.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk.page_content\n",
    "        })\n",
    "\n",
    "    # ê¸°ì¡´ íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ì ì œê±°\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    output_path = os.path.join(dummy_dir, f\"{filename_without_ext}.json\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# PDF ì²˜ë¦¬\n",
    "for filename in pdf_files:\n",
    "    path = os.path.join(data_dir, filename)\n",
    "    process_and_save(path, PyMuPDFLoader)\n",
    "\n",
    "# HWP ì²˜ë¦¬\n",
    "for filename in hwp_files:\n",
    "    path = os.path.join(data_dir, filename)\n",
    "    process_and_save(path, HWPLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a00129",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dir = \"/home/daeseok/AI-Engineer/data/dummy\"  # ì—¬ê¸°ì— ì‹¤ì œ ê²½ë¡œ ë„£ê¸°\n",
    "\n",
    "for filename in sorted(os.listdir(dummy_dir)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        path = os.path.join(dummy_dir, filename)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                print(f\"\\nğŸ“„ {filename}\")\n",
    "                print(f\" - ì²­í¬ ìˆ˜: {len(data)}\")\n",
    "                if data:\n",
    "                    print(f\" - ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:\\n{textwrap.shorten(data[0]['text'], width=200)}\")\n",
    "                else:\n",
    "                    print(\" - ë‚´ìš© ì—†ìŒ (empty list)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ - {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cf9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Chunk 15 / ì´ 326ê°œ\n",
      "============================================================\n",
      "-13- \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ë°ì´í„°ì—°ê³„(API)ì‹œìŠ¤í…œ \n",
      " \n",
      "â—‹ ì •ë³´ê°œë°œíŒ€ \n",
      "í†µí•©ë©”ì„¸ì§•ì‹œìŠ¤í…œ(UMS) \n",
      " \n",
      "â—‹ ë””ì§€í„¸ì „ëµíŒ€ \n",
      "ì›¹ë©”ì¼ì‹œìŠ¤í…œ \n",
      " \n",
      " \n",
      "ì •ë³´ì¸í”„ë¼íŒ€ \n",
      "ì¦ëª…ë°œê¸‰ì‹œìŠ¤í…œ \n",
      " \n",
      "â–³ í•™ìƒì²˜ \n",
      "ëª¨ë°”ì¼í†µí•©(í˜¸ì‡)ì•± \n",
      "Spring boot,  \n",
      "Java, Kotlin, Swift \n",
      "â—‹ ë””ì§€í„¸ì „ëµíŒ€ \n",
      "ëª¨ë°”ì¼ì‹ ë¶„ì¦ \n",
      "Java, Spring \n",
      " \n",
      "í•™ìƒì²˜ \n",
      "ê³ ë ¤ëŒ€ êµí†µ(ì…”í‹€/ì•½ì)ì‹œìŠ¤í…œ \n",
      "node.js \n",
      "â—‹ ë””ì§€í„¸ì „ëµíŒ€\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# í™•ì¸í•  íŒŒì¼ëª…ê³¼ ì²­í¬ ì¸ë±ìŠ¤ ì§€ì •\n",
    "filename = \"ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….json\"\n",
    "chunk_index = 5  # ğŸ” í™•ì¸í•˜ê³  ì‹¶ì€ ì²­í¬ ë²ˆí˜¸\n",
    "\n",
    "# ê²½ë¡œ ì§€ì •\n",
    "dummy_dir = \"/home/daeseok/AI-Engineer/data/dummy\"\n",
    "file_path = os.path.join(dummy_dir, filename)\n",
    "\n",
    "# íŒŒì¼ ë¡œë“œ\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# ì²­í¬ ë²”ìœ„ í™•ì¸\n",
    "if 0 <= chunk_index < len(chunks):\n",
    "    print(f\"\\nğŸ§© Chunk {chunk_index} / ì´ {len(chunks)}ê°œ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(chunks[chunk_index][\"text\"])\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"âŒ ì²­í¬ ë²ˆí˜¸ {chunk_index}ëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (0 ~ {len(chunks)-1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de27ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Chunk 1 / ì´ 87ê°œ\n",
      "============================================================\n",
      "ì¦ëª…ì„œÈƒ 73 15. ì‚¬ì—… ì°¸ì—¬ì¸ë ¥ ëª…ë‹¨Èƒ 74 16. íˆ¬ì… ì¸ë ¥ê³„íšÈƒ 75 17. ê²½ì˜ì‹¤íƒœ í™•ì¸ì„œÈƒ 76 18. í–‰ì • ì²˜ë¶„ í™•ì¸ì„œÈƒ 77 19. ì‚¬íšŒì  ì±…ì„ í™•ì¸ì„œÈƒ 78 20. ê¸°ìˆ ì œì•ˆì„œ í‘œì§€Èƒ 79 21. ê°€ê²©ì œì•ˆì„œÈƒ 80 22. ì œì•ˆì„œ í™•ì¸(ì ‘ìˆ˜)ì¦Èƒ 81 ã€ë¶™ì„ã€‘ 1. ìš©ì—­ì‚¬ì—… ë³´ì•ˆìœ„ê·œ ì²˜ë¦¬ê¸°ì¤€Èƒ82 2. ë³´ì•ˆ ìœ„ì•½ê¸ˆ ë¶€ê³¼ê¸°ì¤€Èƒ84 3. ëˆ„ì¶œê¸ˆì§€ ëŒ€ìƒì •ë³´Èƒ85 4. ì†Œí”„íŠ¸ì›¨ì–´ ë³´ì•ˆì•½ì  ê¸°ì¤€Èƒ86 5. ê¸°ìˆ ì ìš©ê³„íší‘œÈƒ89 6. ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì‚¬ì—…ì˜ ì ì • ì‚¬ì—…ê¸°ê°„ ì¢…í•© ì‚°ì •ì„œâ½Èƒ96 7. ì†Œí”„íŠ¸ì›¨ì–´ì‚¬ì—… ì˜í–¥í‰ê°€ ê²€í† ê²°ê³¼ì„œä·†Èƒ97äµ´â…  ì‚¬ì—… ê°œìš”1. ì‚¬ì—…ê°œìš”  ê°€. ì‚¬ ì—… ëª… : ê´‘ì£¼ì •ì±…ì—°êµ¬ì•„ì¹´ì´ë¸Œ(GPA) ì‹œìŠ¤í…œ ê°œë°œ   ë‚˜. ì‚¬ì—…ê¸°ê°„ : ì°©ìˆ˜ì¼ë¡œë¶€í„° 5ê°œì›”  ë‹¤. ì‚¬ì—…ì˜ˆì‚° : ê¸ˆ43,000,000ì› ì´ë‚´ (ë¶€ê°€ê°€ì¹˜ì„¸ í¬í•¨)  ë¼. ìˆ˜ìš”ê¸°ê´€ : (ì¬)ê´‘ì£¼ì—°êµ¬ì›  ë§ˆ. ì…ì°°ë°©ì‹ ë° ë‚™ì°°ì ê²°ì •ë°©ì‹    1) ì…ì°°ë°©ì‹ : ì œí•œê²½ìŸì…ì°°, ì§€ì—­ì œí•œ(ê´‘ì£¼ê´‘ì—­ì‹œ)    2) ë‚™ì°°ì ê²°ì •ë°©ì‹ : í˜‘ìƒì— ì˜í•œ ê³„ì•½       â‘ ã€Œì§€ë°©ìì¹˜ë‹¨ì²´ë¥¼ ë‹¹ì‚¬ìë¡œ í•˜ëŠ” ê³„ì•½ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹ã€ì œ43ì¡°          (í˜‘ìƒì— ì˜í•œ ê³„ì•½ì²´ê²°)       â‘¡ã€Œì§€ë°©ìì¹˜ë‹¨ì²´ ì…ì°°ì‹œ ë‚™ì°°ì ê²°ì •ê¸°ì¤€ã€ì œ7ì¥ í˜‘ìƒì— ì˜í•œ ê³„ì•½ ì²´ê²° ê¸°ì¤€2. ì¶”ì§„ë°°ê²½ ë° í•„ìš”ì„±  â ê´‘ì£¼ì •ì±…ì—°êµ¬íšŒ ì •ì±…ì •ë³´ ê³µìœ ì²´ê³„ êµ¬ì¶• í•„ìš”  â ê´‘ì£¼ì‹œ ì¶œìì¶œì—°ê¸°ê´€ì˜ ì •ì±…ì—°êµ¬ìë£Œ(ìì²´ ë³´ê³ ì„œ ë° ìœ„ìˆ˜íƒ ë³´ê³ ì„œ í¬í•¨)ë¥¼ í•˜ë‚˜ì˜ ì‚¬ì´íŠ¸ë¡œ ì§‘ì í•˜ì—¬ ê³µìœ í•˜ëŠ” í”Œë«í¼ ìš´ì˜  â ê° ê¸°ê´€ë³„ë¡œ ë°œì£¼í–ˆë˜ ìˆ˜íƒê³¼ì œ ë³´ê³ ì„œë¥¼ í•œ ê³³ì— ëª¨ìœ¼ê³  ê´‘ì£¼ì‹œ ì •ì±…ì—°êµ¬ê²°ê³¼ë¥¼ ì§‘ëŒ€ì„±í•˜ëŠ” ê²°ê³¼ ê¸°ëŒ€ 3. ì‚¬ì—…ë²”ìœ„â–¡ ìš´ì˜ ë°©ì‹  â ê´‘ì£¼ì—°êµ¬ì› : í—ˆë¸Œì‚¬ì´íŠ¸ ê°œë°œ ë° ì„œë¹„ìŠ¤ ìš´ì˜  â íšŒì› ê¸°ê´€ : Open API ê¸°ë°˜ì˜ í”„ë¡œê·¸ë¨ ê°œë°œ êµ¬í˜„  â ì•„ì¹´ì´ë¹™ ë°©ë²• : íšŒì› ê¸°ê´€ë³„ë¡œ ì™„ì„±ëœ ë³´ê³ ì„œë¥¼ Open API í˜•ì‹ìœ¼ë¡œ ì œê³µë°›ì•„ í™ˆí˜ì´ì§€ì— ìë™ì—…ë¡œë“œ ë˜ë„ë¡ ì„œë¹„ìŠ¤ ì œê³µ  â ë‚˜ë¹„ìŠ¤ì™€ í¬ì¸íŠ¸ ë“± ê¸°ì¡´ ì •ì±…ì—°êµ¬ ì•„ì¹´ì´ë¸Œ ì‚¬ì´íŠ¸ëŠ” ë°°ë„ˆë¡œ ì œê³µ  â ë³¸ ì‚¬ì—…ì—ì„œëŠ” ì‹œë²”\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# í™•ì¸í•  íŒŒì¼ëª…ê³¼ ì²­í¬ ì¸ë±ìŠ¤ ì§€ì •\n",
    "filename = \"ì¬ë‹¨ë²•ì¸ ê´‘ì£¼ì—°êµ¬ì›_ê´‘ì£¼ì •ì±…ì—°êµ¬ì•„ì¹´ì´ë¸Œ(GPA) ì‹œìŠ¤í…œ ê°œë°œ.json\"\n",
    "chunk_index = 1  # ğŸ” í™•ì¸í•˜ê³  ì‹¶ì€ ì²­í¬ ë²ˆí˜¸\n",
    "\n",
    "# ê²½ë¡œ ì§€ì •\n",
    "dummy_dir = \"/home/daeseok/AI-Engineer/data/dummy\"\n",
    "file_path = os.path.join(dummy_dir, filename)\n",
    "\n",
    "# íŒŒì¼ ë¡œë“œ\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# ì²­í¬ ë²”ìœ„ í™•ì¸\n",
    "if 0 <= chunk_index < len(chunks):\n",
    "    print(f\"\\nğŸ§© Chunk {chunk_index} / ì´ {len(chunks)}ê°œ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(chunks[chunk_index][\"text\"])\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"âŒ ì²­í¬ ë²ˆí˜¸ {chunk_index}ëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (0 ~ {len(chunks)-1})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
