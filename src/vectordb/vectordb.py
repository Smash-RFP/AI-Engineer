import os
import json
import shutil
import argparse
from tqdm import tqdm

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document

# --- ì„¤ì • (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©) ---
DEFAULT_DUMMY_DATA_DIR = "/home/dlgsueh02/AI-Engineer/data/dummy"
DEFAULT_CHROMA_DB_DIR = "./chroma_db"
COLLECTION_NAME = "rfp_documents"
EMBEDDING_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

def check_api_keys():
    """OpenAI API í‚¤ ì„¤ì • ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        print("âœ… OpenAI API í‚¤ ì„¤ì • ì™„ë£Œ")
    else:
        print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def load_and_parse_documents(source_dir):
    """
    ì§€ì •ëœ ë””ë ‰í„°ë¦¬ì—ì„œ JSONL íŒŒì¼ì„ ì½ì–´ LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_documents = []
    # .jsonl í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    jsonl_files = [f for f in os.listdir(source_dir) if f.endswith(".jsonl")]

    print(f"\nì´ {len(jsonl_files)}ê°œì˜ JSONL íŒŒì¼(RFP)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for filename in tqdm(jsonl_files, desc="JSONL íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        file_path = os.path.join(source_dir, filename)
        docs_in_file_count = 0
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                # íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤. (JSONL í˜•ì‹)
                for line in f:
                    # ë¹ˆ ì¤„ì€ ê±´ë„ˆëœë‹ˆë‹¤.
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        # ê° ì¤„ì„ í•˜ë‚˜ì˜ JSON ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤. (json.loads ì‚¬ìš©)
                        doc_data = json.loads(line)
                        
                        # LangChain Document ê°ì²´ ìƒì„±
                        doc = Document(
                            # === ìˆ˜ì •ëœ ë¶€ë¶„: "page_content" ëŒ€ì‹  "text" í‚¤ì—ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ===
                            page_content=doc_data.get("text", ""), 
                            metadata=doc_data.get("metadata", {})
                        )
                        all_documents.append(doc)
                        docs_in_file_count += 1

                    except json.JSONDecodeError:
                        tqdm.write(f"  - [ê²½ê³ ] {filename} íŒŒì¼ì˜ íŠ¹ì • ì¤„ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. í•´ë‹¹ ì¤„ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            if docs_in_file_count > 0:
                tqdm.write(f"  - [ì„±ê³µ] {filename} ({docs_in_file_count}ê°œ ë¬¸ì„œ ì²˜ë¦¬)")
            else:
                tqdm.write(f"  - [ì •ë³´] {filename} íŒŒì¼ì— ì²˜ë¦¬í•  ë¬¸ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            tqdm.write(f"  - [ì‹¤íŒ¨] {filename}: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")

    print(f"\nâœ… ì´ {len(all_documents)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return all_documents

def add_documents_in_batches(vector_db, documents, batch_size):
    """ë¬¸ì„œë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„° DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    if not documents:
        print("\nâš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(documents)}ê°œì˜ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.")
    
    for i in tqdm(range(0, len(documents), batch_size), desc="DB ì €ì¥ ì¤‘"):
        batch = documents[i:i + batch_size]

        for doc in batch:
            for key, value in doc.metadata.items():
                if isinstance(value, list):
                    # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³ , ì‰¼í‘œì™€ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
                    doc.metadata[key] = ", ".join(map(str, value))
                    
        vector_db.add_documents(batch)

    print("\nâœ… ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

def save_chunk_id_mapping(vector_db, save_path="data/chunk_id_map.json"):
    """Chroma ë‚´ë¶€ doc.idì™€ chunk_idë¥¼ ë§¤í•‘í•˜ì—¬ ì €ì¥"""
    raw_data = vector_db._collection.get(include=["metadatas"])
    ids = raw_data["ids"]              # ë¦¬ìŠ¤íŠ¸ of doc.id
    metadatas = raw_data["metadatas"]  # ë¦¬ìŠ¤íŠ¸ of metadata dicts

    mapping = {
        doc_id: metadata.get("chunk_id", "unknown")
        for doc_id, metadata in zip(ids, metadatas)
    }

    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

    print(f"chunk_id ë§¤í•‘ {len(mapping)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {save_path}")

def run(data_dir, db_dir, rebuild):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if rebuild and os.path.exists(db_dir):
        print(f"ğŸ”„ '{db_dir}' í´ë”ë¥¼ ì‚­ì œí•˜ê³  DBë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
        shutil.rmtree(db_dir)
    os.makedirs(db_dir, exist_ok=True)

    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = Chroma(
        persist_directory=db_dir,
        embedding_function=embedding_model,
        collection_name=COLLECTION_NAME
    )

    documents = load_and_parse_documents(data_dir)
    add_documents_in_batches(vector_db, documents, BATCH_SIZE)
    save_chunk_id_mapping(vector_db)

    print(f"\n--- [DB ìƒíƒœ í™•ì¸] ---")
    count = vector_db._collection.count()
    print(f"ğŸ” í˜„ì¬ DBì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {count}ê°œ")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="JSONL íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument("--data_dir", type=str, default=DEFAULT_DUMMY_DATA_DIR, help="ì…ë ¥ JSONL íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--db_dir", type=str, default=DEFAULT_CHROMA_DB_DIR, help="ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--rebuild", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()
    
    check_api_keys()
    run(args.data_dir, args.db_dir, args.rebuild)