import os
import json

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document

# openai api key
def check_api_keys():
    openai_key = os.getenv("OPENAI_API_KEY")

    if openai_key:
        print("ì„¤ì • ì™„ë£Œ")
    else:
        print("ì„¤ì • ì•ˆë¨")

# ì„¤ì •
DUMMY_DATA_DIR = "/home/dlgsueh02/AI-Engineer/data/dummy"
CHROMA_DB_DIR = "./chroma_db"
COLLECTION_NAME = "rfp_documents"
EMBEDDING_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

# JSON íŒŒì¼ ë¡œë“œ ë° ë¬¸ì„œ ê°ì²´ ìƒì„± í•¨ìˆ˜
def load_and_parse_documents(source_dir):
    all_documents = []
    json_files = [f for f in os.listdir(DUMMY_DATA_DIR) if f.endswith(".json")]

    print(f"\nì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼(RFP)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for filename in json_files:
        file_path = os.path.join(DUMMY_DATA_DIR, filename)
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

                # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì—ì„œ 'chunks' í‚¤ë¥¼ ì´ìš©í•´ ì‹¤ì œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
                chunks_data = data.get("chunks", []) # .get()ì„ ì‚¬ìš©í•´ 'chunks' í‚¤ê°€ ì—†ì–´ë„ ì˜¤ë¥˜ ì—†ì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

                if not chunks_data:
                    print(f"  - [ê²½ê³ ] {filename} íŒŒì¼ì— ì²˜ë¦¬í•  ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                # ê° ì²­í¬ë¥¼ LangChainì˜ Document ê°ì²´ë¡œ ë³€í™˜
                for chunk in chunks_data:
                    # 'section' ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                    metadata = {
                        "source": data.get("filename", filename), # JSON ì•ˆì˜ íŒŒì¼ëª…ì„ ìš°ì„  ì‚¬ìš©
                        "chunk_id": chunk.get("chunk_id"),
                        "section": chunk.get("section", "N/A")    # section ì •ë³´ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„
                    }
                    
                    doc = Document(
                        page_content=chunk.get("text", ""), # textê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
                        metadata=metadata
                    )
                    all_documents.append(doc)
                
                print(f"  - [ì„±ê³µ] {filename} ({len(chunks_data)}ê°œ ì²­í¬)")

        except json.JSONDecodeError:
            print(f"  - [ì‹¤íŒ¨] {filename} íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        except Exception as e:
            print(f"  - [ì‹¤íŒ¨] {filename} ì²˜ë¦¬ ì¤‘ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"\nì´ {len(all_documents)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œ ì¡°ê°(ì²­í¬)ì„ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.")


#  ë²¡í„° DBì— ë°ì´í„° ì €ì¥ í•¨ìˆ˜
def add_documents_in_batches(vector_db, all_documents, batch_size):
    if all_documents:
    # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ê°œìˆ˜ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ)
        batch_size = 100 
        
        print(f"\nì´ {len(all_documents)}ê°œì˜ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        for i in range(0, len(all_documents), batch_size):
            # all_documents ë¦¬ìŠ¤íŠ¸ì—ì„œ batch_sizeë§Œí¼ ìŠ¬ë¼ì´ì‹±
            batch = all_documents[i:i + batch_size]
            
            # ìŠ¬ë¼ì´ì‹±ëœ 'batch'ë§Œ DBì— ì¶”ê°€
            vector_db.add_documents(batch)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"  - {i + len(batch)} / {len(all_documents)} ì²˜ë¦¬ ì™„ë£Œ")

        print("\nâœ… ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    else:
        print("\nâš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DUMMY_DATA_DIR ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")


# êµ¬ì¶•ëœ ë²¡í„° DB í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_vector_db_search(vector_db, query):
    print("\n--- [ê²€ìƒ‰ í…ŒìŠ¤íŠ¸] ---")
    # ì‹¤ì œ RFP ë‚´ìš©ê³¼ ê´€ë ¨ ìˆì„ ë²•í•œ ì§ˆë¬¸
    query = "ì°¨ì„¸ëŒ€ í¬í„¸ ì‹œìŠ¤í…œì˜ ì£¼ìš” ê¸°ëŠ¥ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€?"

    # retrieverê°€ ì‚¬ìš©í•  ìœ ì‚¬ë„ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    # k=3: ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ 3ê°œë¥¼ ê°€ì ¸ì˜´
    retrieved_docs = vector_db.similarity_search(query, k=3)

    if retrieved_docs:
        print(f"â“ ì§ˆë¬¸: \"{query}\"")
        print(f"\nğŸ” ê²€ìƒ‰ëœ ìœ ì‚¬ ì²­í¬ Top {len(retrieved_docs)}ê°œ:")
        print("-" * 60)
        for i, doc in enumerate(retrieved_docs):
            print(f"[{i+1}] ì¶œì²˜: {doc.metadata.get('source', 'N/A')} (ì²­í¬ ID: {doc.metadata.get('chunk_id', 'N/A')})")
            print(f"    - ì„¹ì…˜: {doc.metadata.get('section', 'N/A')}")
            print(f"    - ë‚´ìš©: {doc.page_content[:200]}...") # ë‚´ìš©ì´ ê¸°ë¯€ë¡œ 200ìë§Œ ì¶œë ¥
            print("-" * 60)
    else:
        print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():

    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embedding_model,
        collection_name=COLLECTION_NAME
    )

    all_documents = load_and_parse_documents(DUMMY_DATA_DIR)

    add_documents_in_batches(vector_db, all_documents, BATCH_SIZE)

    test_query = "ì°¨ì„¸ëŒ€ í¬í„¸ ì‹œìŠ¤í…œì˜ ì£¼ìš” ê¸°ëŠ¥ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€?"
    test_vector_db_search(vector_db, test_query)

if __name__ == "__main__":
    check_api_keys()
    main()