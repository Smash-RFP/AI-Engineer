import os
import json

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document

# openai api key
def check_api_keys():
    openai_key = os.getenv("OPENAI_API_KEY")

    if openai_key:
        print("ì„¤ì •")
    else:
        print("ì„¤ì • ì•ˆë¨")

if __name__ == "__main__":
    check_api_keys()
    

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
DUMMY_DATA_DIR = "/home/dlgsueh02/AI-Engineer/data/dummy"
CHROMA_DB_DIR = "./chroma_db"

# ì„ë² ë”© ëª¨ë¸
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„° DB
vector_db = Chroma(
    persist_directory=CHROMA_DB_DIR,
    embedding_function=embedding_model,
    collection_name="rfp_documents"
)

# JSON íŒŒì¼ ë¡œë“œ ë° ë¬¸ì„œ ê°ì²´ ìƒì„±
all_documents = []
json_files = [f for f in os.listdir(DUMMY_DATA_DIR) if f.endswith(".json")]

print(f"\nì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼(RFP)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

for filename in json_files:
    file_path = os.path.join(DUMMY_DATA_DIR, filename)
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

            # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì—ì„œ 'chunks' í‚¤ë¥¼ ì´ìš©í•´ ì‹¤ì œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
            chunks_data = data.get("chunks", []) # .get()ì„ ì‚¬ìš©í•´ 'chunks' í‚¤ê°€ ì—†ì–´ë„ ì˜¤ë¥˜ ì—†ì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

            if not chunks_data:
                print(f"  - [ê²½ê³ ] {filename} íŒŒì¼ì— ì²˜ë¦¬í•  ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # ê° ì²­í¬ë¥¼ LangChainì˜ Document ê°ì²´ë¡œ ë³€í™˜
            for chunk in chunks_data:
                # 'section' ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                metadata = {
                    "source": data.get("filename", filename), # JSON ì•ˆì˜ íŒŒì¼ëª…ì„ ìš°ì„  ì‚¬ìš©
                    "chunk_id": chunk.get("chunk_id"),
                    "section": chunk.get("section", "N/A")    # section ì •ë³´ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„
                }
                
                doc = Document(
                    page_content=chunk.get("text", ""), # textê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
                    metadata=metadata
                )
                all_documents.append(doc)
            
            print(f"  - [ì„±ê³µ] {filename} ({len(chunks_data)}ê°œ ì²­í¬)")

    except json.JSONDecodeError:
        print(f"  - [ì‹¤íŒ¨] {filename} íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    except Exception as e:
        print(f"  - [ì‹¤íŒ¨] {filename} ì²˜ë¦¬ ì¤‘ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

print(f"\nì´ {len(all_documents)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œ ì¡°ê°(ì²­í¬)ì„ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.")

#  ë²¡í„° DBì— ë°ì´í„° ì €ì¥
if all_documents:
    # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ê°œìˆ˜ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ)
    batch_size = 100 
    
    print(f"\nì´ {len(all_documents)}ê°œì˜ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for i in range(0, len(all_documents), batch_size):
        # all_documents ë¦¬ìŠ¤íŠ¸ì—ì„œ batch_sizeë§Œí¼ ìŠ¬ë¼ì´ì‹±
        batch = all_documents[i:i + batch_size]
        
        # ìŠ¬ë¼ì´ì‹±ëœ 'batch'ë§Œ DBì— ì¶”ê°€
        vector_db.add_documents(batch)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        print(f"  - {i + len(batch)} / {len(all_documents)} ì²˜ë¦¬ ì™„ë£Œ")

    print("\nâœ… ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

else:
    print("\nâš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DUMMY_DATA_DIR ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# êµ¬ì¶•ëœ ë²¡í„° DB í…ŒìŠ¤íŠ¸
print("\n--- [ê²€ìƒ‰ í…ŒìŠ¤íŠ¸] ---")
# ì‹¤ì œ RFP ë‚´ìš©ê³¼ ê´€ë ¨ ìˆì„ ë²•í•œ ì§ˆë¬¸
query = "ì°¨ì„¸ëŒ€ í¬í„¸ ì‹œìŠ¤í…œì˜ ì£¼ìš” ê¸°ëŠ¥ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€?"

# retrieverê°€ ì‚¬ìš©í•  ìœ ì‚¬ë„ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
# k=3: ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ 3ê°œë¥¼ ê°€ì ¸ì˜´
retrieved_docs = vector_db.similarity_search(query, k=3)

if retrieved_docs:
    print(f"â“ ì§ˆë¬¸: \"{query}\"")
    print(f"\nğŸ” ê²€ìƒ‰ëœ ìœ ì‚¬ ì²­í¬ Top {len(retrieved_docs)}ê°œ:")
    print("-" * 60)
    for i, doc in enumerate(retrieved_docs):
        print(f"[{i+1}] ì¶œì²˜: {doc.metadata.get('source', 'N/A')} (ì²­í¬ ID: {doc.metadata.get('chunk_id', 'N/A')})")
        print(f"    - ì„¹ì…˜: {doc.metadata.get('section', 'N/A')}")
        print(f"    - ë‚´ìš©: {doc.page_content[:200]}...") # ë‚´ìš©ì´ ê¸°ë¯€ë¡œ 200ìë§Œ ì¶œë ¥
        print("-" * 60)
else:
    print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")