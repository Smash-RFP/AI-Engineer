import os
import re
import json
import shutil
import argparse
from tqdm import tqdm

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.docstore.document import Document

# --- ì„¤ì • (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©) ---
DEFAULT_DUMMY_DATA_DIR = "/home/gcp-JeOn-8/RFP_A/data2/output_jsonl"
DEFAULT_CHROMA_DB_DIR = "./chroma_db"  # DB ë””ë ‰í„°ë¦¬ ì´ë¦„ ë³€ê²½
COLLECTION_NAME = "rfp_documents"      # ì»¬ë ‰ì…˜ ì´ë¦„ ë³€ê²½

# --- 1. ì‚¬ìš©í•  HuggingFace ëª¨ë¸ ì§€ì • ---
EMBEDDING_MODEL = "BAAI/bge-m3"
BATCH_SIZE = 8

def normalize_source_id(source_id: str) -> str:
    text = source_id.replace(".json", "")
    text = re.sub(r"[()]", "", text)
    text = re.sub(r"[^\w]", "_", text)
    text = re.sub(r"_+", "_", text)
    return text.strip("_")

def load_and_parse_documents(source_dir):
    """
    ì§€ì •ëœ ë””ë ‰í„°ë¦¬ì—ì„œ JSONL íŒŒì¼ì„ ì½ì–´ LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_documents = []
    jsonl_files = [f for f in os.listdir(source_dir) if f.endswith(".jsonl")]

    print(f"\nì´ {len(jsonl_files)}ê°œì˜ JSONL íŒŒì¼(RFP)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for filename in tqdm(jsonl_files, desc="JSONL íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        file_path = os.path.join(source_dir, filename)
        docs_in_file_count = 0
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        doc_data = json.loads(line)
                        
                        metadata = doc_data.get("metadata", {})
                        
                        source_id = normalize_source_id(metadata.get("source_id", ""))
                        metadata["source_id"] = source_id
                        
                        keywords = metadata.get("keywords", "")
                    
                        if isinstance(keywords, list):
                            keywords = ", ".join(map(str, keywords))

                        metadata["meta_text"] = f"{source_id} {keywords}".strip()
                        
                        doc = Document(
                            page_content=doc_data.get("text", ""), 
                            metadata=metadata  # ìˆ˜ì •ëœ ë©”íƒ€ë°ì´í„°ë¥¼ ì „ë‹¬
                        )

                        all_documents.append(doc)
                        docs_in_file_count += 1

                    except json.JSONDecodeError:
                        tqdm.write(f"  - [ê²½ê³ ] {filename} íŒŒì¼ì˜ íŠ¹ì • ì¤„ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. í•´ë‹¹ ì¤„ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            if docs_in_file_count > 0:
                tqdm.write(f"  - [ì„±ê³µ] {filename} ({docs_in_file_count}ê°œ ë¬¸ì„œ ì²˜ë¦¬)")
            else:
                tqdm.write(f"  - [ì •ë³´] {filename} íŒŒì¼ì— ì²˜ë¦¬í•  ë¬¸ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            tqdm.write(f"  - [ì‹¤íŒ¨] {filename}: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")

    print(f"\nâœ… ì´ {len(all_documents)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return all_documents

def add_documents_in_batches(vector_db, documents, batch_size):
    """ë¬¸ì„œë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„° DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    if not documents:
        print("\nâš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(documents)}ê°œì˜ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.")
    
    for i in tqdm(range(0, len(documents), batch_size), desc="DB ì €ì¥ ì¤‘"):
        batch = documents[i:i + batch_size]

        for doc in batch:
            for key, value in doc.metadata.items():
                if isinstance(value, list):
                    # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³ , ì‰¼í‘œì™€ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
                    doc.metadata[key] = ", ".join(map(str, value))

        vector_db.add_documents(batch)

    print("\n ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")   


def save_chunk_id_mapping(vector_db, save_path = f"/gcp-JeOn-8/RFP_A/data2/chunk_id_map.json"):
    """Chroma ë‚´ë¶€ doc.idì™€ chunk_idë¥¼ ë§¤í•‘í•˜ì—¬ ì €ì¥"""
    raw_data = vector_db._collection.get(include=["metadatas"])
    ids = raw_data["ids"]
    metadatas = raw_data["metadatas"]

    mapping = {
        doc_id: metadata.get("chunk_id", "unknown")
        for doc_id, metadata in zip(ids, metadatas)
    }

    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

    print(f"chunk_id ë§¤í•‘ {len(mapping)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {save_path}")

def run_A(data_dir, db_dir, rebuild):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if rebuild and os.path.exists(db_dir):
        print(f"ğŸ”„ '{db_dir}' í´ë”ë¥¼ ì‚­ì œí•˜ê³  DBë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
        shutil.rmtree(db_dir)
    os.makedirs(db_dir, exist_ok=True)
    
    # --- 2. HuggingFaceEmbeddings ê°ì²´ ìƒì„± ---
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}, # E5 ëª¨ë¸ì€ normalize_embeddingsë¥¼ Trueë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    )
    
    vector_db = Chroma(
        persist_directory=db_dir,
        embedding_function=embedding_model,
        collection_name=COLLECTION_NAME
    )

    doc_count = vector_db._collection.count()

    if doc_count > 0 and not rebuild:
        print(f"\n DBê°€ ì´ë¯¸ '{db_dir}'ì— ì¡´ì¬í•˜ë©° {doc_count}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("   ìƒˆë¡œ êµ¬ì¶•í•˜ë ¤ë©´ --rebuild í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        print(f"\n--- [DB ìƒíƒœ í™•ì¸] ---")
        print(f"ğŸ” í˜„ì¬ DBì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {doc_count}ê°œ")
        return  # í•¨ìˆ˜ë¥¼ ì—¬ê¸°ì„œ ì¢…ë£Œí•˜ì—¬ ì•„ë˜ì˜ êµ¬ì¶• ê³¼ì •ì„ ìƒëµ

    documents = load_and_parse_documents(data_dir)
    add_documents_in_batches(vector_db, documents, BATCH_SIZE)
    save_chunk_id_mapping(vector_db)

    print(f"\n--- [DB ìƒíƒœ í™•ì¸] ---")
    count = vector_db._collection.count()
    print(f"ğŸ” í˜„ì¬ DBì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {count}ê°œ")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="JSONL íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument("--data_dir", type=str, default=DEFAULT_DUMMY_DATA_DIR, help="ì…ë ¥ JSONL íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--db_dir", type=str, default=DEFAULT_CHROMA_DB_DIR, help="ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ")
    parser.add_argument("--rebuild", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()
    
    run_A(args.data_dir, args.db_dir, args.rebuild)