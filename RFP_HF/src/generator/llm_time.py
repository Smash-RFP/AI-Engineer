import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from vllm import LLM, SamplingParams

MODEL_PATH = "/home/NEUL77/AI-Engineer/data/merged_qwen3_8b"
PROMPT = "ë‹¤ìŒ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ 2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\nëŒ€í•œë¯¼êµ­ì€ 21ì„¸ê¸° ë“¤ì–´ AIì™€ ë°˜ë„ì²´ ì‚°ì—…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê¸€ë¡œë²Œ ê²½ìŸë ¥ì„ ê°•í™”í•˜ê³  ìˆë‹¤."
MAX_NEW_TOKENS = 128

def measure_hf():
    print("ğŸ”„ [HF] ë¡œë“œ ì¤‘...")
    t0 = time.time()
    tok = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH, device_map="auto", torch_dtype="auto", low_cpu_mem_usage=True
    )
    gen = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tok,
        return_full_text=False,
        do_sample=False,
        max_new_tokens=MAX_NEW_TOKENS
    )
    load_s = time.time() - t0
    print(f"âœ… [HF] ë¡œë“œ {load_s:.2f}s")

    # ì›Œë°ì—…
    _ = gen(PROMPT)

    # ë³¸ ì¸¡ì •
    t1 = time.time()
    out = gen(PROMPT)[0]["generated_text"]
    t2 = time.time()
    # ìƒì„± í† í° ìˆ˜ ì¶”ì •
    n_new = len(tok.encode(out))
    tokps = n_new / (t2 - t1)
    print(f"âš™ï¸  [HF] ìƒì„± {t2 - t1:.2f}s, í† í°/ì´ˆ â‰ˆ {tokps:.2f}")
    return load_s, t2 - t1, tokps

def measure_vllm():
    print("ğŸ”„ [vLLM] ë¡œë“œ ì¤‘...")
    t0 = time.time()
    llm = LLM(
        model=MODEL_PATH,
        dtype="float16",
        # ì´ˆê¸°í™” ì˜¤ë²„í—¤ë“œ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”í•œ ë§Œí¼ë§Œ
        max_model_len=4096,
        gpu_memory_utilization=0.9,
        tensor_parallel_size=1,
        quantization="bitsandbytes",  # í•„ìš” ì—†ë‹¤ë©´ ì œê±°í•´ ë³´ì„¸ìš” (ì´ˆê¸°í™” ë‹¨ì¶• ê°€ëŠ¥)
        trust_remote_code=True,
    )
    load_s = time.time() - t0
    print(f"âœ… [vLLM] ë¡œë“œ {load_s:.2f}s")

    sampling = SamplingParams(
        temperature=0.0, top_p=1.0, max_tokens=MAX_NEW_TOKENS
    )

    # ì›Œë°ì—…
    _ = llm.generate([PROMPT], sampling)

    # ë³¸ ì¸¡ì •
    t1 = time.time()
    outputs = llm.generate([PROMPT], sampling)
    t2 = time.time()
    text = outputs[0].outputs[0].text
    # vLLMì€ í† í¬ë‚˜ì´ì €ë¥¼ ì•ˆ ë…¸ì¶œí•˜ë¯€ë¡œ ëŒ€ëµ ê¸¸ì´ë¡œ ì¶”ì •í•˜ê±°ë‚˜ HF í† í¬ë‚˜ì´ì €ë¡œ ë‹¤ì‹œ ì¸ì½”ë”©
    tok = AutoTokenizer.from_pretrained(MODEL_PATH)
    n_new = len(tok.encode(text))
    tokps = n_new / (t2 - t1)
    print(f"âš™ï¸  [vLLM] ìƒì„± {t2 - t1:.2f}s, í† í°/ì´ˆ â‰ˆ {tokps:.2f}")
    return load_s, t2 - t1, tokps

if __name__ == "__main__":
    hf_load, hf_gen, hf_tokps = measure_hf()
    print()
    vllm_load, vllm_gen, vllm_tokps = measure_vllm()

    print("\nğŸ“Š ìš”ì•½")
    print(f"[HF]   ë¡œë“œ {hf_load:.2f}s | ìƒì„± {hf_gen:.2f}s | tok/s â‰ˆ {hf_tokps:.2f}")
    print(f"[vLLM] ë¡œë“œ {vllm_load:.2f}s | ìƒì„± {vllm_gen:.2f}s | tok/s â‰ˆ {vllm_tokps:.2f}")